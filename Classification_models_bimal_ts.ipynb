{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFu+VDOZdeEcfbLA4BQs16",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bimal-bp/Machine_learning_1.0_with_Bimal_TS/blob/ML_MODELS/Classification_models_bimal_ts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification models: logistic regression, k-Nearest Neighbors, decision trees, ensemble methods (Random Forests, Gradient Boosting Machines)\n",
        "\n",
        "- Classification models are powerful tools for predicting outcomes and making decisions based on data. üîçüìä\n",
        "\n",
        "- Logistic regression is a popular method for binary classification, where the goal is to predict one of two possible outcomes. It models the probability of the outcome as a function of the input variables. üîçüìä\n",
        "\n",
        "- K-Nearest Neighbors is a simple but effective method for classification that looks at the k-nearest neighbors to the input point and assigns it the most common class among them. It can handle both binary and multi-class classification problems. üåüü§ñ\n",
        "\n",
        "- Decision trees are a popular method for classification and can handle both binary and multi-class classification problems. They work by recursively splitting the data based on the input variables until a certain stopping criterion is met. üå≥ü§î\n",
        "\n",
        "- Ensemble methods like Random Forests and Gradient Boosting Machines combine multiple decision trees to improve the accuracy and performance of the model. Random Forests use bagging to generate multiple trees, while Gradient Boosting Machines use boosting to iteratively improve the performance of the mode\n",
        "-  Bagging: üéí Learn how to combine multiple models to create a diverse ensemble that improves predictions. Discover how the aggregated decision-making of bagged models can reduce overfitting and enhance generalization.\n",
        "-  Boosting: üöÄ Unleash the potential of boosting algorithms to create a strong predictive model. Explore how boosting iteratively trains weak models to form a powerful ensemble. Uncover the secrets behind adaptive boosting and gradient boosting."
      ],
      "metadata": {
        "id": "9QuNtJDxrE1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic regression** :\n",
        "\n",
        "- Logistic regression is a popular statistical method used for binary classification tasks, where the goal is to predict one of two possible outcomes.\n",
        "\n",
        "-  This function maps any real-valued input to the range of [0, 1]\n",
        "\n",
        "- Logistic regression is a powerful tool for many real-world applications, including fraud detection, marketing, and healthcare.\n",
        "\n",
        "code :\n",
        "- from sklearn.linear_model import LogisticRegression\n",
        "- lor = LogisticRegression(penalty='none',solver='sag')\n",
        "- lor.fit(X,y)\n",
        "- print(lor.coef_)\n",
        "-  [[4.7808362 0.2062583]]\n",
        "- print(lor.intercept_)\n",
        "-\n",
        "[5.7492783]\n"
      ],
      "metadata": {
        "id": "_ryj77rBsPFw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "True/False**\n",
        "1. Logistic regression is used for multi-class classification tasks. (True/False)\n",
        "2. The logistic function maps input values to the range [0, 1]. (True/False)\n",
        "3. Logistic regression is not suitable for binary classification tasks. (True/False)\n",
        "\n",
        "Answers**\n",
        "1. False\n",
        "2. True\n",
        "3. False"
      ],
      "metadata": {
        "id": "WrK6gNJLtty4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "polynomial-logistic-regression.\n",
        "\n",
        "- from sklearn.preprocessing import PolynomialFeatures\n",
        "- poly = PolynomialFeatures(degree=3,include_bias=False)\n",
        "- X_trf = poly.fit_transform(X)\n",
        "- it has degree function which gives a good result on a certain number by increaseing its reduce also"
      ],
      "metadata": {
        "id": "07QjQqBWubFQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**k-Nearest Neighbors** :\n",
        "\n",
        "- K-Nearest Neighbors (KNN) is a popular machine learning algorithm used for classification and regression tasks. It is a simple and intuitive algorithm that works by looking at the k-nearest neighbors to the input point and assigning it the most common class or average value among them.\n",
        "- KNN is a non-parametric algorithm, which means it does not make any assumptions about the underlying distribution of the data. Instead, it stores the entire training dataset and uses it for prediction. This makes KNN a versatile algorithm that can handle complex decision boundaries and is robust to noisy data\n",
        "-Once we have identified the k-nearest neighbors, we use their classes or values to make the prediction. For example, in a binary classification task, we would assign the most common class among the k-nearest neighbors to the input point. In a regression task, we would assign the average value among the k-nearest neighbors to the input point.\n",
        "\n",
        "code :\n",
        "\n",
        "- from sklearn.neighbors import KNeighborsClassifier\n",
        "- knn = KNeighborsClassifier(n_neighbors=3)\n",
        "- knn.fit(X_train,y_train)\n",
        "\n",
        "- You can change the degree values from 1 to n and check in which degree u are gettting high accuracy thats ur n value\n",
        "- but normally we take odd values to get correct answer\n",
        "- https://www.kaggle.com/code/campusx/knn-on-breast-cancer-dataset"
      ],
      "metadata": {
        "id": "-iD2v3oGva2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decision trees**\n",
        "\n",
        "- Decision trees are a popular machine learning algorithm used for classification and regression tasks. They work by breaking down the data into smaller and smaller subsets, while at the same time creating a tree-like structure of decisions and their possible consequences\n",
        "- The final outcomes are represented by the leaves of the tree. Decision trees can handle both numerical and categorical data and are particularly useful for dealing with high-dimensional datasets\n",
        "\n",
        "code\n",
        "- from sklearn.tree import DecisionTreeClassifier\n",
        "- clf=DecisionTreeClassifier()\n",
        "- clf.fit(X_train,y_train\n",
        "- y_pred=clf.predict(X_test)\n",
        "- https://github.com/campusx-official/decision-trees/blob/master/Decision%20Tree%20Classification%20Demo.ipynb"
      ],
      "metadata": {
        "id": "N9CyK_5tz9dH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Ensemble methods (Random Forests, Gradient Boosting Machines)**\n",
        "\n",
        "- Ensemble methods are machine learning techniques that combine multiple models to improve prediction accuracy and robustness. Two popular ensemble methods are Random Forests and Gradient Boosting Machines. üå≥üöÄ\n",
        "\n",
        "- Random Forests combine multiple decision trees to make predictions. Each tree is trained on a random subset of the data, and the final prediction is the average prediction of all the trees. This helps to reduce overfitting and improve accuracy. üå≤ü§ùüßî\n",
        "\n",
        "- Gradient Boosting Machines, on the other hand, build models in a sequential manner, with each new model attempting to correct the errors of the previous model. This is done by assigning weights to the data points based on their previous error. The final prediction is the weighted sum of all the models. üî¢üï∞Ô∏è\n",
        "\n",
        "- Both Random Forests and Gradient Boosting Machines are powerful machine learning algorithms that can handle large and complex datasets. They can be used for both regression and classification tasks and have many real-world applications, such as in finance, marketing, and healthcare\n",
        "\n",
        "- Here we have 3,4 set of d-tress with differnt data set so it called forest\n",
        "\n",
        "- Random forest- https://github.com/campusx-official/100-days-of-machine-learning/blob/main/day65-random-forest/code-example-random-forest.ipynb\n",
        "- sampling - https://github.com/campusx-official/100-days-of-machine-learning/blob/main/day65-random-forest/rf_learning_tool.ipynb"
      ],
      "metadata": {
        "id": "NcLUGaHy5ScI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradient-boosting**\n",
        "\n",
        "- https://github.com/campusx-official/100-days-of-machine-learning/blob/main/gradient-boosting/gradient_boost_step_by_step.ipynb\n",
        "\n",
        "ada boost -\n",
        "\n",
        "- https://github.com/campusx-official/100-days-of-machine-learning/blob/main/adaboost_demo.ipynb"
      ],
      "metadata": {
        "id": "47Y0z7It7hS3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LOVE WITH BIMAL_TS**"
      ],
      "metadata": {
        "id": "gEd1mBRw8jpc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gXshMQ8uz4LK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n7DTG4dnzVHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e_aDQaw8vg3I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}